from dragonmapper import hanzi, transcriptions
import jieba
import pandas as pd
import re
import requests 
import spacy
from spacy_streamlit import visualize_ner, visualize_tokens
#from spacy.language import Language
from spacy.tokens import Doc
import streamlit as st

# Global variables
MODELS = {"ä¸­æ–‡": "zh_core_web_sm", 
          "English": "en_core_web_sm", 
          "æ—¥æœ¬èªž": "ja_ginza"}
models_to_display = list(MODELS.keys())
ZH_TEXT = """ï¼ˆä¸­å¤®ç¤¾ï¼‰è¿ŽæŽ¥è™Žå¹´åˆ°ä¾†ï¼Œå°åŒ—101ä»Šå¤©è¡¨ç¤ºï¼Œå³æ—¥èµ·æŽ¨å‡ºã€Œè™Žå¹´æ–°æ˜¥ç‡ˆå…‰ç§€ã€ï¼Œå°‡æŒçºŒè‡³2æœˆ5æ—¥ï¼Œæ¯æ™š6æ™‚è‡³10æ™‚ï¼Œé™¤æ•´é»žæœƒæœ‰å ±æ™‚ç‡ˆå…‰è®ŠåŒ–å¤–ï¼Œæ¯15åˆ†é˜é‚„æœƒæœ‰3åˆ†é˜çš„ç‡ˆå…‰ç§€ã€‚å°åŒ—101ä¸‹åˆé€éŽæ–°èžç¨¿è¡¨ç¤ºï¼Œä»Šå¹´ç‰¹åˆ¥è¨­è¨ˆã€Œè™Žå¹´æ–°æ˜¥ç‡ˆå…‰ç§€ã€ï¼Œå¾žä»Šæ™šé–‹å§‹é–ƒè€€å°åŒ—å¤©éš›ç·šï¼Œä¸€ç›´å»¶çºŒè‡³2æœˆ5æ—¥ï¼Œå…±7å¤©ã€‚"""
ZH_REGEX = "\d{2,4}[\u4E00-\u9FFF]+"
EN_TEXT = """(CNN) Not all Lunar New Year foods are created equal. Some only make a brief appearance at the festival for auspicious purposes. Others are so delicious they grace dim sum tables around the world all year.
Turnip cake -- called "loh bak goh" in Cantonese -- falls into the latter category.
Chef Tsang Chiu King, culinary director of Ming Court in Hong Kong's Wan Chai area, has his own theory on why turnip cake is such a popular Lunar New Year dish, especially in southern China.
"Compared to other Lunar New Year cakes, turnip cake is popular as it's one of the few savory new year puddings. Together with the freshness of the white radish, it can be quite addictive as a snack or a main dish," he says."""
EN_REGEX = "(ed|ing)$"
JA_TEXT = """ï¼ˆæœæ—¥æ–°èžï¼‰å¯…ï¼ˆã¨ã‚‰ï¼‰å¹´ã®2022å¹´ã‚’å‰ã«ã€90ç¨®480åŒ¹ã®é‡Žç”Ÿå‹•ç‰©ã‚’é£¼è‚²ã™ã‚‹ã€Œåˆ°æ´¥ï¼ˆã„ã¨ã†ã¥ï¼‰ã®æ£®å…¬åœ’ã€ï¼ˆåŒ—ä¹å·žå¸‚ï¼‰ãŒç››ã‚Šä¸ŠãŒã£ã¦ã„ã‚‹ã€‚åŒåœ’ã®ãƒžã‚¹ã‚³ãƒƒãƒˆã¯ã‚¢ãƒ ãƒ¼ãƒ«ãƒˆãƒ©ã®ãƒŸãƒ©ã‚¤ï¼ˆé›Œã€10æ­³ï¼‰ã€‚22å¹´ã¯ã€Œãƒ‹ãƒ£ãƒ¼ãƒ‹ãƒ£ãƒ¼ã€ã®å¹´ã¨ã—ã¦ãƒã‚³å¥½ãã®é–“ã§è©±é¡Œã¨ãªã£ã¦ãŠã‚Šã€ã€Œå¹²æ”¯ï¼ˆãˆã¨ï¼‰ã§å”¯ä¸€ã®ãƒã‚³ç§‘ã®ãƒˆãƒ©äººæ°—ã«ã¤ãªãŒã‚Œã°ã€ã¨æœŸå¾…ã—ã¦ã„ã‚‹ã€‚"""
JA_REGEX = "[ãŸã„]$"
DESCRIPTION = "AIæ¨¡åž‹è¼”åŠ©èªžè¨€å­¸ç¿’"
TOK_SEP = " | "

# Custom tokenizer class
class JiebaTokenizer:
    def __init__(self, vocab):
        self.vocab = vocab

    def __call__(self, text):
        words = jieba.cut(text) # returns a generator
        tokens = list(words) # convert the genetator to a list
        spaces = [False] * len(tokens)
        doc = Doc(self.vocab, words=tokens, spaces=spaces)
        return doc
    
# Utility functions
def create_jap_df(tokens):
    seen_texts = []
    filtered_tokens = []
    for tok in tokens:
        if tok.text not in seen_texts:
            filtered_tokens.append(tok)
            
    df = pd.DataFrame(
      {
          "å–®è©ž": [tok.orth_ for tok in filtered_tokens],
          "ç™¼éŸ³": ["/".join(tok.morph.get("Reading")) for tok in filtered_tokens],
          "è©žå½¢è®ŠåŒ–": ["/".join(tok.morph.get("Inflection")) for tok in filtered_tokens],
          "åŽŸå½¢": [tok.lemma_ for tok in filtered_tokens],
          #"æ­£è¦å½¢": [tok.norm_ for tok in verbs],
      }
    )
    st.dataframe(df)
    csv = df.to_csv().encode('utf-8')
    st.download_button(
      label="ä¸‹è¼‰è¡¨æ ¼",
      data=csv,
      file_name='jap_forms.csv',
      )

def filter_tokens(doc):
    clean_tokens = [tok for tok in doc if tok.pos_ not in ["PUNCT", "SYM"]]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_email]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_url]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_num]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_punct]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_space]
    return clean_tokens
            
def moedict_caller(word):
    st.write(f"### {word}")
    req = requests.get(f"https://www.moedict.tw/a/{word}.json")
    if req:
        with st.expander("é»žæ“Š + æª¢è¦–çµæžœ"):
            st.json(req.json())
    else:
        st.write("æŸ¥ç„¡çµæžœ")
          
# Page setting
st.set_page_config(
    page_icon="ðŸ¤ ",
    layout="wide",
)

# Choose a language model
st.markdown(f"# {DESCRIPTION}") 
st.markdown("## èªžè¨€æ¨¡åž‹") 
selected_model = st.radio("è«‹é¸æ“‡èªžè¨€", models_to_display)
nlp = spacy.load(MODELS[selected_model])
          
# Merge entity spans to tokens
# nlp.add_pipe("merge_entities") 
st.markdown("---")

# Default text and regex
st.markdown("## å¾…åˆ†æžæ–‡æœ¬") 
if selected_model == models_to_display[0]: # Chinese
    # Select a tokenizer if the Chinese model is chosen
    selected_tokenizer = st.radio("è«‹é¸æ“‡æ–·è©žæ¨¡åž‹", ["spaCy", "jieba-TW"])
    if selected_tokenizer == "jieba-TW":
        nlp.tokenizer = JiebaTokenizer(nlp.vocab)
    default_text = ZH_TEXT
    default_regex = ZH_REGEX
elif selected_model == models_to_display[1]: # English
    default_text = EN_TEXT 
    default_regex = EN_REGEX 
elif selected_model == models_to_display[2]: # Japanese
    default_text = JA_TEXT
    default_regex = JA_REGEX 

st.info("ä¿®æ”¹æ–‡æœ¬å¾Œï¼ŒæŒ‰ä¸‹Ctrl + Enterä»¥æ›´æ–°")
text = st.text_area("",  default_text)
doc = nlp(text)
st.markdown("---")

# Two columns
left, right = st.columns(2)

with left:
    # Model output
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©žç‰¹å¾µ")
    st.markdown("---")

with right:
    punct_and_sym = ["PUNCT", "SYM"]
    if selected_model == models_to_display[0]: # Chinese 
        st.markdown("## åˆ†æžå¾Œæ–‡æœ¬") 
        for idx, sent in enumerate(doc.sents):
            tokens_text = [tok.text for tok in sent if tok.pos_ not in punct_and_sym]
            pinyins = [hanzi.to_pinyin(word) for word in tokens_text]
            display = []
            for text, pinyin in zip(tokens_text, pinyins):
                res = f"{text} [{pinyin}]"
                display.append(res)
            display_text = TOK_SEP.join(display)
            st.write(f"{idx+1} >>> {display_text}")
        
        st.markdown("## å–®è©žè§£é‡‹")
        clean_tokens = filter_tokens(doc)
        alphanum_pattern = re.compile(r"[a-zA-Z0-9]")
        clean_tokens_text = [tok.text for tok in clean_tokens if not alphanum_pattern.search(tok.text)]
        vocab = list(set(clean_tokens_text))
        if vocab:
            selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©ž: ", vocab, vocab[0:3])
            for w in selected_words:
                moedict_caller(w)                        
                    
    elif selected_model == models_to_display[2]: # Japanese 
        st.markdown("## åˆ†æžå¾Œæ–‡æœ¬") 
        for idx, sent in enumerate(doc.sents):
            clean_tokens = [tok for tok in sent if tok.pos_ not in ["PUNCT", "SYM"]]
            tokens_text = [tok.text for tok in clean_tokens]
            readings = ["/".join(tok.morph.get("Reading")) for tok in clean_tokens]
            display = [f"{text} [{reading}]" for text, reading in zip(tokens_text, readings)]
            display_text = TOK_SEP.join(display)
            st.write(f"{idx+1} >>> {display_text}")          
        
        st.markdown("## è©žå½¢è®ŠåŒ–")
        # Collect inflected forms
        inflected_forms = [tok for tok in doc if tok.tag_.startswith("å‹•è©ž") or tok.tag_.startswith("å½¢")]
        if inflected_forms:
            create_jap_df(inflected_forms)

    else:
        st.write("Work in progress")
