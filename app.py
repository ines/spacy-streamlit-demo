from dragonmapper import hanzi, transcriptions
import jieba
import pandas as pd
import requests 
import spacy
from spacy_streamlit import visualize_ner, visualize_tokens
from spacy.tokens import Doc
import streamlit as st

# Global variables
MODELS = {"ä¸­æ–‡": "zh_core_web_sm", 
          "English": "en_core_web_sm", 
          "æ—¥æœ¬èª": "ja_ginza"}
models_to_display = list(MODELS.keys())
ZH_TEXT = "ï¼ˆä¸­å¤®ç¤¾ï¼‰ä¸­å¤®æµè¡Œç–«æƒ…æŒ‡æ®ä¸­å¿ƒå®£å¸ƒï¼Œä»Šå¤©åœ‹å…§æ–°å¢60ä¾‹COVID-19ï¼ˆ2019å† ç‹€ç—…æ¯’ç–¾ç—…ï¼‰ï¼Œåˆ†åˆ¥ç‚º49ä¾‹å¢ƒå¤–ç§»å…¥ï¼Œ11ä¾‹æœ¬åœŸç—…ä¾‹ï¼Œæ˜¯å»å¹´8æœˆ29æ—¥æœ¬åœŸæ–°å¢13ä¾‹ä»¥ä¾†çš„æ–°é«˜ï¼Œåˆæ­¥ç ”åˆ¤å…¶ä¸­10ä¾‹å€‹æ¡ˆçš†èˆ‡æ¡ƒåœ’æ©Ÿå ´ç–«æƒ…æœ‰é—œã€‚"
MOEDICT_URL = "https://www.moedict.tw/uni/"
ZH_REGEX = "\d{2,4}"
EN_TEXT = "(CNN) Covid-19 hospitalization rates among children are soaring in the United States, with an average of 4.3 children under 5 per 100,000 hospitalized with an infection as of the week ending January 1, up from 2.6 children the previous week, according to data from the US Centers for Disease Control and Prevention. This represents a 48% increase from the week ending December 4, and the largest increase in hospitalization rate this age group has seen over the course of the pandemic."
EN_REGEX = "(ed|ing)$"
JA_TEXT = "ï¼ˆæœæ—¥æ–°èï¼‰æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã®å›½å†…æ„ŸæŸ“è€…ã¯9æ—¥ã€æ–°ãŸã«8249äººãŒç¢ºèªã•ã‚ŒãŸã€‚2æ—¥é€£ç¶šã§8åƒäººã‚’è¶…ãˆãŸã®ã¯æ˜¨å¹´9æœˆ11æ—¥ä»¥æ¥ã€ç´„4ã‚«æœˆã¶ã‚Šã€‚å…¨å›½çš„ã«æ„ŸæŸ“æ‹¡å¤§ãŒé€²ã‚€ä¸­ã€å¹´ã‚’ã¾ãŸã„ã 1é€±é–“ã®æ„ŸæŸ“è€…ã®éåŠæ•°ãŒ30ä»£ä»¥ä¸‹ã ã£ãŸã€‚ã‚³ãƒ­ãƒŠç‰¹æªæ³•ã«åŸºã¥ãã€Œã¾ã‚“å»¶é˜²æ­¢ç­‰é‡ç‚¹æªç½®ã€ãŒ9æ—¥ã‹ã‚‰é©ç”¨ã•ã‚ŒãŸ3çœŒã§ã¯ã€åºƒå³¶ã§éå»æœ€å¤šã®619äººãŒç¢ºèªã•ã‚ŒãŸã€‚"
JA_REGEX = "[ãŸã„]$"
DESCRIPTION = "spaCyè‡ªç„¶èªè¨€è™•ç†æ¨¡å‹å±•ç¤º"
TOK_SEP = " | "

# Custom tokenizer class
class JiebaTokenizer:
    def __init__(self, vocab):
        self.vocab = vocab

    def __call__(self, text):
        words = jieba.cut(text) # returns a generator
        tokens = list(words) # convert the genetator to a list
        spaces = [False] * len(tokens)
        doc = Doc(self.vocab, words=tokens, spaces=spaces)
        return doc

st.set_page_config(
    page_icon="ğŸ¤ ",
    layout="wide",
)

# Choose a language model
st.markdown(f"# {DESCRIPTION}") 
st.markdown("## èªè¨€æ¨¡å‹") 
selected_model = st.radio("è«‹é¸æ“‡èªè¨€", models_to_display)
nlp = spacy.load(MODELS[selected_model])
          
# Merge entity spans to tokens
# nlp.add_pipe("merge_entities") 
st.markdown("---")

# Default text and regex
st.markdown("## å¾…åˆ†ææ–‡æœ¬") 
if selected_model == models_to_display[0]: # Chinese
    # Select a tokenizer if the Chinese model is chosen
    selected_tokenizer = st.radio("è«‹é¸æ“‡æ–·è©æ¨¡å‹", ["jieba-TW", "spaCy"])
    if selected_tokenizer == "jieba-TW":
        nlp.tokenizer = JiebaTokenizer(nlp.vocab)
    default_text = ZH_TEXT
    default_regex = ZH_REGEX
elif selected_model == models_to_display[1]: # English
    default_text = EN_TEXT 
    default_regex = EN_REGEX 
elif selected_model == models_to_display[2]: # Japanese
    default_text = JA_TEXT
    default_regex = JA_REGEX 

text = st.text_area("",  default_text)
doc = nlp(text)
st.markdown("---")

# Two columns
left, right = st.columns(2)

with left:
    # Model output
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©ç‰¹å¾µ")
    st.markdown("---")

with right:
    tokens = [tok.text for tok in doc]
    if selected_model == models_to_display[0]: # Chinese 
        spaced_tokens = TOK_SEP.join(tokens)
        pinyin = hanzi.to_pinyin(spaced_tokens)
        st.markdown("## Original text with words seperated by |") 
        st.write(spaced_tokens)
        st.markdown("## Pinyin") 
        st.write(pinyin)
        verbs = [tok.text for tok in doc if tok.pos_ == "VERB"]
        if verbs:
            st.markdown("## Verbs")
            selected_verbs = st.multiselect("Select verbs to look up", verbs, verbs[0:1])
            for v in selected_verbs:
                st.write(f"### {v}")
                res = requests.get(MOEDICT_URL+v)
                if res:
                    with st.expander("Click on + to see details."):
                        st.json(res.json())
                else:
                    st.write("No result")
            
        nouns = [tok.text for tok in doc if tok.pos_ == "NOUN"]
        if nouns:
            st.markdown("## Nouns")
            selected_nouns = st.multiselect("Select nouns to look up", nouns, nouns[0:1])
            for n in selected_nouns:
                st.write(f"### {n}")
                res = requests.get(MOEDICT_URL+n)
                if res:
                    with st.expander("Click on + to see details."):
                        st.json(res.json())
                else:
                    st.write("No result")
                    
    elif selected_model == models_to_display[2]: # Japanese 
        st.markdown("## åŸæ–‡èˆ‡ç™¼éŸ³") 
        readings = [str(tok.morph.get("Reading")) for tok in doc]
        text_with_readings = [tok+reading for tok, reading in zip(tokens, readings)]
        text_with_readings = TOK_SEP.join(text_with_readings)
        st.write(text_with_readings)

        verbs = [tok for tok in doc if tok.pos_ == "VERB"]
        if verbs:
            st.markdown("## å‹•è©")
            df = pd.DataFrame(
                {
                    "å–®è©": [tok.orth_ for tok in verbs],
                    "ç™¼éŸ³": [tok.morph.get("Reading") for tok in verbs],
                    "è©å½¢è®ŠåŒ–": [tok.morph.get("Inflection") for tok in verbs],
                    "åŸå½¢": [tok.lemma_ for tok in verbs],
                    #"æ­£è¦å½¢": [tok.norm_ for tok in verbs],
                }
            )
            st.dataframe(df)
            
        auxes = [tok for tok in doc if tok.pos_ == "AUX"]
        if auxes:
            st.markdown("## åŠ©å‹•è©")
            df = pd.DataFrame(
                {
                    "å–®è©": [tok.orth_ for tok in auxes],
                    "ç™¼éŸ³": [tok.morph.get("Reading") for tok in auxes],
                    "è©å½¢è®ŠåŒ–": [tok.morph.get("Inflection") for tok in auxes],
                    "åŸå½¢": [tok.lemma_ for tok in auxes],
                    #"æ­£è¦å½¢": [tok.norm_ for tok in auxes],
                }
            )
            st.dataframe(df)

    else:
          st.write("Work in progress")
