from dragonmapper import hanzi, transcriptions
import jieba
import pandas as pd
import requests 
import spacy
from spacy_streamlit import visualize_ner, visualize_tokens
#from spacy.language import Language
from spacy.tokens import Doc
import streamlit as st

# Global variables
MODELS = {"ä¸­æ–‡": "zh_core_web_sm", 
          "English": "en_core_web_sm", 
          "æ—¥æœ¬èªž": "ja_ginza"}
models_to_display = list(MODELS.keys())
ZH_TEXT = "ï¼ˆä¸­å¤®ç¤¾ï¼‰ä¸­å¤®æµè¡Œç–«æƒ…æŒ‡æ®ä¸­å¿ƒå®£å¸ƒï¼Œä»Šå¤©åœ‹å…§æ–°å¢ž60ä¾‹COVID-19ï¼ˆ2019å† ç‹€ç—…æ¯’ç–¾ç—…ï¼‰ï¼Œåˆ†åˆ¥ç‚º49ä¾‹å¢ƒå¤–ç§»å…¥ï¼Œ11ä¾‹æœ¬åœŸç—…ä¾‹ï¼Œæ˜¯åŽ»å¹´8æœˆ29æ—¥æœ¬åœŸæ–°å¢ž13ä¾‹ä»¥ä¾†çš„æ–°é«˜ï¼Œåˆæ­¥ç ”åˆ¤å…¶ä¸­10ä¾‹å€‹æ¡ˆçš†èˆ‡æ¡ƒåœ’æ©Ÿå ´ç–«æƒ…æœ‰é—œã€‚"
ZH_REGEX = "\d{2,4}"
EN_TEXT = "(CNN) Covid-19 hospitalization rates among children are soaring in the United States, with an average of 4.3 children under 5 per 100,000 hospitalized with an infection as of the week ending January 1, up from 2.6 children the previous week, according to data from the US Centers for Disease Control and Prevention. This represents a 48% increase from the week ending December 4, and the largest increase in hospitalization rate this age group has seen over the course of the pandemic."
EN_REGEX = "(ed|ing)$"
JA_TEXT = "ï¼ˆæœæ—¥æ–°èžï¼‰æ–°åž‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã®å›½å†…æ„ŸæŸ“è€…ã¯9æ—¥ã€æ–°ãŸã«8249äººãŒç¢ºèªã•ã‚ŒãŸã€‚2æ—¥é€£ç¶šã§8åƒäººã‚’è¶…ãˆãŸã®ã¯æ˜¨å¹´9æœˆ11æ—¥ä»¥æ¥ã€ç´„4ã‚«æœˆã¶ã‚Šã€‚å…¨å›½çš„ã«æ„ŸæŸ“æ‹¡å¤§ãŒé€²ã‚€ä¸­ã€å¹´ã‚’ã¾ãŸã„ã 1é€±é–“ã®æ„ŸæŸ“è€…ã®éŽåŠæ•°ãŒ30ä»£ä»¥ä¸‹ã ã£ãŸã€‚ã‚³ãƒ­ãƒŠç‰¹æŽªæ³•ã«åŸºã¥ãã€Œã¾ã‚“å»¶é˜²æ­¢ç­‰é‡ç‚¹æŽªç½®ã€ãŒ9æ—¥ã‹ã‚‰é©ç”¨ã•ã‚ŒãŸ3çœŒã§ã¯ã€åºƒå³¶ã§éŽåŽ»æœ€å¤šã®619äººãŒç¢ºèªã•ã‚ŒãŸã€‚"
JA_REGEX = "[ãŸã„]$"
DESCRIPTION = "spaCyè‡ªç„¶èªžè¨€è™•ç†è¼”åŠ©èªžè¨€å­¸ç¿’"
TOK_SEP = " | "

# Custom tokenizer class
class JiebaTokenizer:
    def __init__(self, vocab):
        self.vocab = vocab

    def __call__(self, text):
        words = jieba.cut(text) # returns a generator
        tokens = list(words) # convert the genetator to a list
        spaces = [False] * len(tokens)
        doc = Doc(self.vocab, words=tokens, spaces=spaces)
        return doc
    
# Utility functions
def create_jap_df(tokens):
          df = pd.DataFrame(
              {
                  "å–®è©ž": [tok.orth_ for tok in tokens],
                  "ç™¼éŸ³": ["/".join(tok.morph.get("Reading")) for tok in tokens],
                  "è©žå½¢è®ŠåŒ–": ["/".join(tok.morph.get("Inflection")) for tok in tokens],
                  "åŽŸå½¢": [tok.lemma_ for tok in tokens],
                  #"æ­£è¦å½¢": [tok.norm_ for tok in verbs],
              }
          )
          st.dataframe(df)
          csv = df.to_csv().encode('utf-8')
          st.download_button(
              label="ä¸‹è¼‰è¡¨æ ¼",
              data=csv,
              file_name='jap_forms.csv',
              )

def filter_tokens(doc):
    clean_tokens = [tok for tok in doc if tok.pos_ not in ["PUNCT", "SYM"]]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_email]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_url]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_num]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_punct]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_space]
    return clean_tokens
            
def moedict_caller(word):
    st.write(f"### {word}")
    req = requests.get(f"https://www.moedict.tw/a/{word}.json")
    if req:
        with st.expander("é»žæ“Š + æª¢è¦–çµæžœ"):
            st.json(req.json())
    else:
        st.write("æŸ¥ç„¡çµæžœ")
          
# Page setting
st.set_page_config(
    page_icon="ðŸ¤ ",
    layout="wide",
)

# Choose a language model
st.markdown(f"# {DESCRIPTION}") 
st.markdown("## èªžè¨€æ¨¡åž‹") 
selected_model = st.radio("è«‹é¸æ“‡èªžè¨€", models_to_display)
nlp = spacy.load(MODELS[selected_model])
          
# Merge entity spans to tokens
# nlp.add_pipe("merge_entities") 
st.markdown("---")

# Default text and regex
st.markdown("## å¾…åˆ†æžæ–‡æœ¬") 
if selected_model == models_to_display[0]: # Chinese
    # Select a tokenizer if the Chinese model is chosen
    selected_tokenizer = st.radio("è«‹é¸æ“‡æ–·è©žæ¨¡åž‹", ["spaCy", "jieba-TW"])
    if selected_tokenizer == "jieba-TW":
        nlp.tokenizer = JiebaTokenizer(nlp.vocab)
    default_text = ZH_TEXT
    default_regex = ZH_REGEX
elif selected_model == models_to_display[1]: # English
    default_text = EN_TEXT 
    default_regex = EN_REGEX 
elif selected_model == models_to_display[2]: # Japanese
    default_text = JA_TEXT
    default_regex = JA_REGEX 

st.info("ä¿®æ”¹æ–‡æœ¬å¾ŒæŒ‰ä¸‹Ctrl + Enteræ›´æ–°")
text = st.text_area("",  default_text)
doc = nlp(text)
st.markdown("---")

# Two columns
left, right = st.columns(2)

with left:
    # Model output
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©žç‰¹å¾µ")
    st.markdown("---")

with right:
    punct_and_sym = ["PUNCT", "SYM"]
    if selected_model == models_to_display[0]: # Chinese 
        st.markdown("## åˆ†æžå¾Œæ–‡æœ¬") 
        for idx, sent in enumerate(doc.sents):
            tokens_text = [tok.text for tok in sent if tok.pos_ not in punct_and_sym]
            pinyins = [hanzi.to_pinyin(word) for word in tokens_text]
            display = []
            for text, pinyin in zip(tokens_text, pinyins):
                res = f"{text} [{pinyin}]"
                display.append(res)
            display_text = TOK_SEP.join(display)
            st.write(f"{idx+1} >>> {display_text}")
        
        st.markdown("## å–®è©žè§£é‡‹")
        clean_tokens = filter_tokens(doc)
        clean_tokens_text = [tok.text for tok in clean_tokens if not tok.is_ascii]
        vocab = list(set(clean_tokens_text))
        if vocab:
            selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©ž: ", vocab, vocab[0:3])
            for w in selected_words:
                moedict_caller(w)                        
                    
    elif selected_model == models_to_display[2]: # Japanese 
        st.markdown("## åˆ†æžå¾Œæ–‡æœ¬") 
        for idx, sent in enumerate(doc.sents):
            clean_tokens = [tok for tok in sent if tok.pos_ not in ["PUNCT", "SYM"]]
            tokens_text = [tok.text for tok in clean_tokens]
            readings = ["/".join(tok.morph.get("Reading")) for tok in clean_tokens]
            display = [f"{text} [{reading}]" for text, reading in zip(tokens_text, readings)]
            display_text = TOK_SEP.join(display)
            st.write(f"{idx+1} >>> {display_text}")          
        
        # tag_ seems to be more accurate than pos_
        verbs = [tok for tok in doc if tok.tag_.startswith("å‹•è©ž")]
        verbs = list(set(verbs))
        if verbs:
            st.markdown("## å‹•è©ž")
            create_jap_df(verbs)
          
        adjs = [tok for tok in doc if tok.pos_ == "ADJ"]
        adjs = list(set(adjs))
        if adjs:
            st.markdown("## å½¢å®¹è©ž")
            create_jap_df(adjs)

    else:
        st.write("Work in progress")
